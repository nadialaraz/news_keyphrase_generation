# -*- coding: utf-8 -*-
"""GUI golden kp untuk 3 input dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WeukxgHj5_kb_r4GWXC-1o3rSmY3OiLw

## **Mount Google Drive & Setup Folder**
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/jiacheng-ye/kg_one2set.git
# %cd /content/kg_one2set

""" ## **Copy Model & Vocab ke Direktori**"""

# Untuk skenario 1
!mkdir -p /content/kg_one2set/scenario_1/vocab_dir
!cp /content/drive/MyDrive/UNSRI/SKRIPSI/OUTPUT_EKSPERIMEN/untuk_GUI/best_scenario_1/best_model.pt /content/kg_one2set/scenario_1/best_model.pt
!cp /content/drive/MyDrive/UNSRI/SKRIPSI/OUTPUT_EKSPERIMEN/untuk_GUI/best_scenario_1/vocab.pt /content/kg_one2set/scenario_1/vocab_dir/vocab.pt

# Untuk skenario 2
!mkdir -p /content/kg_one2set/scenario_2/vocab_dir
!cp /content/drive/MyDrive/UNSRI/SKRIPSI/OUTPUT_EKSPERIMEN/untuk_GUI/best_scenario_2/best_model.pt /content/kg_one2set/scenario_2/best_model.pt
!cp /content/drive/MyDrive/UNSRI/SKRIPSI/OUTPUT_EKSPERIMEN/untuk_GUI/best_scenario_2/vocab.pt /content/kg_one2set/scenario_2/vocab_dir/vocab.pt

"""##**Install Dependencies**"""

!pip install streamlit
!pip install pyngrok

"""## **Predict and Generate Keyphrase**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile predict.py
# 
# import torch
# import numpy as np
# from pykp.model import Seq2SeqModel
# from utils.data_loader import load_vocab, build_data_loader
# from pykp.utils.io import build_interactive_predict_dataset
# from utils.functions import common_process_opt
# 
# def process_opt(opt):
#     opt = common_process_opt(opt)
#     if torch.cuda.is_available() and opt.gpuid >= 0:
#         opt.device = torch.device(f"cuda:{opt.gpuid}")
#     else:
#         opt.device = torch.device("cpu")
#     return opt
# 
# def init_pretrained_model(opt):
#     model = Seq2SeqModel(opt)
#     model.load_state_dict(torch.load(opt.model, map_location=opt.device))
#     model.to(opt.device)
#     model.eval()
#     return model
# 
# def prepare_input(title, abstract):
#     title = title.replace('\n', ' ').strip()
#     abstract = abstract.replace('\n', ' ').strip()
#     return f"{title} <eos> {abstract}"
# 
# def generate_keyphrases(input_text, opt):
#     vocab = load_vocab(opt)
#     src_word_list = input_text.strip().split(' ')
#     tokenized_src = [src_word_list]
#     test_data = build_interactive_predict_dataset(tokenized_src, opt, mode='one2many', include_original=True)
#     test_loader = build_data_loader(test_data, opt, shuffle=False, load_train=False)
#     model = init_pretrained_model(opt)
#     from inference.set_generator import SetGenerator
#     generator = SetGenerator.from_opt(model, opt)
#     batch = next(iter(test_loader))
#     batch = [x.to(opt.device) if hasattr(x, 'to') else x for x in batch]
# 
#     # unpack batch sesuai yang diharapkan SetGenerator.inference
#     src, src_lens, src_mask, src_oov, oov_lists, src_str, *_ = batch
# 
#     with torch.no_grad():
#         result = generator.inference(
#             src, src_lens, src_oov, src_mask, oov_lists, opt.vocab['word2idx']
#         )
#         # result["predictions"] adalah tensor berisi token id
#         # flatten predictions agar jadi 1D list of int
#         predictions = np.array(result["predictions"][0].cpu()).flatten().tolist()
#         # convert ke list of phrase
#         idx2word = opt.vocab['idx2word']
#         max_kp_len = opt.max_kp_len
#         keyphrases = []
#         for i in range(0, len(predictions), max_kp_len):
#             phrase_ids = predictions[i:i + max_kp_len]
#             # stop di first <null> atau <pad> atau <eos>
#             phrase = []
#             for idx in phrase_ids:
#                 word = idx2word[idx]
#                 if word in ['<null>', '<pad>', '<eos>']:
#                     break
#                 phrase.append(word)
#             phrase_str = ' '.join(phrase).strip()
#             if len(phrase) >= 1 and phrase_str and phrase_str not in keyphrases:
#                 keyphrases.append(phrase_str)
# 
#     # Hilangkan duplikat tapi urutan tetap
#     seen = set()
#     keyphrases_final = []
#     for kp in keyphrases:
#         if kp and kp != '‚àÖ' and kp not in seen:
#             keyphrases_final.append(kp)
#             seen.add(kp)
# 
#     return keyphrases_final

"""## **Keyphrase Generator and Separator (Present and Absent)**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile generate.py
# 
# import argparse
# from predict import process_opt, generate_keyphrases, prepare_input
# 
# class KeyphraseGenerator:
#     def __init__(self, model_path, vocab_path, beam_size=2, gpuid=0):
#         parser = argparse.ArgumentParser()
#         parser.add_argument('--src_file', type=str, default='input.txt', help='Input file containing text for prediction')
#         parser.add_argument('--model', type=str, default='/content/kg_one2set/best_model.pt', help='Path to the trained model file')
#         parser.add_argument('--vocab', type=str, default='/content/kg_one2set/vocab_dir', help='Path to vocab directory (should contain vocab.pt)')
#         parser.add_argument('--beam_size', type=int, default=2, help='Beam size for beam search decoding')
#         parser.add_argument('--fix_kp_num_len', action='store_true', default=True, help='Use set generator (ONE2SET paradigm)')
#         parser.add_argument('--remove_title_eos', action='store_true', default=True, help='Remove <eos> after title')
#         parser.add_argument('--one2many', action='store_true', default=True, help='Set mode to one2many')
#         parser.add_argument('--pred_path', type=str, default='predictions/', help='Prediction output directory')
#         parser.add_argument('--gpuid', type=int, default=0, help='GPU id to use (default=0 for cuda, -1 for cpu)')
#         parser.add_argument('--exp_path', type=str, default='.', help='Experiment directory')
#         parser.add_argument('--seed', type=int, default=9527, help='Random seed')
#         parser.add_argument('--vocab_size', type=int, default=50000, help='Vocabulary size')
#         parser.add_argument('--max_unk_words', type=int, default=2000, help='Maximum number of unknown words')
#         parser.add_argument('--batch_size', type=int, default=8, help='Batch size for prediction')
#         parser.add_argument('--batch_workers', type=int, default=0, help='Number of workers for generating batches')
# 
#         # ARSITEKTUR MODEL (lihat model.py, encoder, decoder)
#         parser.add_argument('--word_vec_size', type=int, default=512, help='Embedding dimension')
#         parser.add_argument('--enc_layers', type=int, default=6, help='Number of encoder layers')
#         parser.add_argument('--dec_layers', type=int, default=6, help='Number of decoder layers')
#         parser.add_argument('--dropout', type=float, default=0.1, help='Dropout probability')
#         parser.add_argument('--d_model', type=int, default=512, help='Model dimension')
#         parser.add_argument('--n_head', type=int, default=8, help='Number of attention heads')
#         parser.add_argument('--dim_ff', type=int, default=2048, help='Feedforward dimension')
# 
#         # ONE2SET / ONE2MANY CONTROL
#         parser.add_argument('--max_kp_len', type=int, default=6, help='Maximum length of keyphrase')
#         parser.add_argument('--max_kp_num', type=int, default=20, help='Number of control codes/keyphrases')
#         parser.add_argument('--copy_attention', action='store_true', default=True, help='Use copy mechanism')
#         parser.add_argument('--seperate_pre_ab', action='store_true', default=True, help='Use separate set loss for present/absent')
# 
#         # DECODE/UTILITY ARGUMEN TAMBAHAN
#         parser.add_argument('--block_ngram_repeat', type=int, default=0, help='Block repeat of n-gram')
#         parser.add_argument('--n_best', type=int, default=1, help='Number of best sequences to keep')
#         parser.add_argument('--max_length', type=int, default=6, help='Maximum prediction length')
#         parser.add_argument('--length_penalty_factor', type=float, default=0., help='Length penalty parameter')
#         parser.add_argument('--coverage_penalty_factor', type=float, default=0., help='Coverage penalty parameter')
#         parser.add_argument('--length_penalty', default='none', choices=['none', 'wu', 'avg'], help='Length Penalty to use.')
#         parser.add_argument('--coverage_penalty', default='none', choices=['none', 'wu', 'summary'], help='Coverage Penalty to use.')
#         parser.add_argument('--replace_unk', action='store_true', default=True, help='Replace the unk token with token of highest attention')
#         parser.add_argument('--log_path', type=str, default="logs", help='Log path')
# 
#         self.opt, _ = parser.parse_known_args()
#         self.opt.model = model_path
#         self.opt.vocab = vocab_path
#         self.opt.beam_size = beam_size
#         self.opt.gpuid = gpuid
#         self.opt = process_opt(self.opt)
# 
#     def generate(self, title, abstract):
#         input_text = prepare_input(title, abstract)
#         keyphrases = generate_keyphrases(input_text, self.opt)
#         return keyphrases
# 
#     @staticmethod
#     def separate_keyphrases(input_text, keyphrases):
#         input_text_low = input_text.lower()
#         present = []
#         absent = []
#         for kp in keyphrases:
#             tokens = kp.lower().split()
#             if all(token in input_text_low for token in tokens):
#                 present.append(kp)
#             else:
#                 absent.append(kp)
#         return present, absent

"""## **View the UI**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import time
# import streamlit as st
# import pandas as pd
# from generate import KeyphraseGenerator
# 
# st.set_page_config(
#     page_title="Keyphrase Generator",
#     page_icon="üß†",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )
# 
# st.markdown("""
#     <style>
#         body {background-color: #F5F7FA;}
#         .main {background-color: #ffffff; border-radius: 10px; padding: 2rem;}
#         .stButton>button {background-color: #6C63FF; color: white; font-weight: bold; border-radius: 10px; padding: 10px 24px; font-size: 18px;}
#         .stButton>button:hover {background-color: #574b90;}
#         .highlight-box {background-color: #E0F7FA; border-left: 6px solid #00BCD4; padding: 12px; border-radius: 8px; margin-top: 10px;}
#     </style>
# """, unsafe_allow_html=True)
# 
# st.title("üì∞ Keyphrase Generator for News Article")
# st.markdown("#### üöÄ Powered by ONE2SET Transformer Model")
# 
# col1, col2 = st.columns([3, 1], gap="large")
# 
# model_options = {
#     "Scenario 1 (Best F1@5)": {
#         "model": "/content/kg_one2set/scenario_1/best_model.pt",
#         "vocab": "/content/kg_one2set/scenario_1/vocab_dir",
#         "info": [
#             "assign_step=2",
#             "learning_rate=0.0001",
#             "beam_size=2"
#         ]
#     },
#     "Scenario 2 (Best F1@M)": {
#         "model": "/content/kg_one2set/scenario_2/best_model.pt",
#         "vocab": "/content/kg_one2set/scenario_2/vocab_dir",
#         "info": [
#             "assign_step=3",
#             "learning_rate=0.0001",
#             "beam_size=2"
#         ]
#     }
# }
# 
# with col1:
#     with st.form("input_form"):
#         # PILIH SKENARIO/MODEL
#         selected_scenario = st.selectbox(
#             "Choose Model Scenario",
#             list(model_options.keys()),
#             help="Select the model you want to use for keyphrase generation."
#         )
#         title = st.text_input("News Title", placeholder="Enter the title here...")
#         abstract = st.text_area("News Content", placeholder="Enter the content here...", height=180)
#         submit = st.form_submit_button("‚ú® Generate Keyphrases")
# 
# with col2:
#     st.markdown("#### üß† Model Loaded")
#     st.markdown(f"""
#     <div class="highlight-box" style="word-break: break-all;">
#         <b style="font-size:16px;">Scenario:</b><br>
#         <span style="font-size:15px; color:#444;">{selected_scenario}</span>
#         <hr style="margin:6px 0;">
#         <b style="font-size:16px;">Model Experiment:</b><br>
#         <ul style="font-size:13px; color:#666; margin-top:2px; margin-bottom:2px;">
#             <li>{model_options[selected_scenario]['info'][0]}</li>
#             <li>{model_options[selected_scenario]['info'][1]}</li>
#             <li>{model_options[selected_scenario]['info'][2]}</li>
#         </ul>
#     </div>
#     """, unsafe_allow_html=True)
# 
# st.markdown("----")
# 
# # ===== 3 DATA INPUT GOLDEN KEYPHRASE =====
# golden_inputs = [
#     {
#         "title": "The G.O.P. Campaign Message in a Word: Jobs, Jobs, Jobs",
#         "keyphrases": ['Unemployment', 'Elections', 'Obama Barack', 'Republican Party', 'United States Economy', 'Economic Conditions and Trends', 'House of Representatives', 'United States Politics and Government'],
#         "prmu": ['P', 'P', 'M', 'R', 'M', 'M', 'R', 'M']
#     },
#     {
#         "title": "China Ranks Last of 65 Nations in Internet Freedom",
#         "keyphrases": ['China', 'Freedom House', 'Censorship', 'Computers and the Internet', 'Tech Industry', 'Freedom of speech'],
#         "prmu": ['P', 'P', 'P', 'M', 'U', 'R']
#     },
#     {
#         "title": "Couple Given Home Confinement for Morgan Stanley Insider Trading",
#         "keyphrases": ['Morgan Stanley', 'Insider Trading', 'Decisions and Verdicts'],
#         "prmu": ['P', 'P', 'M']
#     }
# ]
# 
# def match_golden_input(title):
#     # Normalisasi dan cocokan judul dengan golden_inputs
#     for item in golden_inputs:
#         if title.strip() == item["title"].strip():
#             # Klasifikasikan present/absent berdasarkan PRMU
#             present = [kp for kp, p in zip(item["keyphrases"], item["prmu"]) if p.upper() == "P"]
#             absent  = [kp for kp, p in zip(item["keyphrases"], item["prmu"]) if p.upper() in ["R", "M", "U"]]
#             return present, absent
#     return None, None
# 
# if submit:
#     if not title or not abstract:
#         st.warning("Title and Content are required!")
#     else:
#         model_path = model_options[selected_scenario]["model"]
#         vocab_path = model_options[selected_scenario]["vocab"]
#         with st.spinner("üîÑ Generating keyphrases..."):
#             generator = KeyphraseGenerator(model_path, vocab_path)
#             keyphrases = generator.generate(title, abstract)
#             present, absent = generator.separate_keyphrases(f"{title} {abstract}", keyphrases)
# 
#         # ====== GOLDEN KEYPHRASE TABLE ======
#         golden_present, golden_absent = match_golden_input(title)
#         if golden_present is not None or golden_absent is not None:
#             st.markdown("### üèÖ Golden Keyphrase (Ground Truth)")
#             maxlen_golden = max(len(golden_present), len(golden_absent))
#             golden_present += [""] * (maxlen_golden - len(golden_present))
#             golden_absent  += [""] * (maxlen_golden - len(golden_absent))
#             golden_df = pd.DataFrame({"Present Keyphrase": golden_present, "Absent Keyphrase": golden_absent})
#             st.table(golden_df)
# 
#         st.success("‚úÖ Keyphrases generated!")
#         st.markdown("### üìå Predicted Keyphrases")
#         st.code("; ".join(keyphrases))
#         st.markdown("### üîë Present and Absent Keyphrases")
#         maxlen = max(len(present), len(absent))
#         present += [""] * (maxlen - len(present))
#         absent += [""] * (maxlen - len(absent))
#         df = pd.DataFrame({"Present Keyphrase": present, "Absent Keyphrase": absent})
#         st.table(df)
# 
# st.caption("Made with ‚ù§Ô∏è by ONE2SET, powered by Streamlit.")

"""## **Run Streamlit App with Ngrok**"""

from pyngrok import ngrok
ngrok.set_auth_token("2zOdVaSR6CYgrt3e3vvpFenr08l_2tcr7jTX4uc3SNDwWYupg")

from pyngrok import ngrok

# Jalankan streamlit di background
import os
os.system('streamlit run app.py &')

# Akses public link
public_url = ngrok.connect(8501)
print("Streamlit Ngrok URL:", public_url)